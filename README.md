# ğŸ“ˆ Stock News Scraper

A Dockerized Python-based news scraping pipeline that collects real-time financial headlines and stores them in PostgreSQL *and* locally for quick inspection.

## ğŸ“‚ Project Structure

```
stock-news-scraper/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ news.json               # Latest scraped output JSON file
â”œâ”€â”€ Docker/
â”‚   â”œâ”€â”€ Dockerfile              # Production Docker build for scraper
â”‚   â””â”€â”€ requirements.txt        # Runtime Python dependencies
â”œâ”€â”€ postgresql/
â”‚   â”œâ”€â”€ init.sql                # Database schema and setup
â”‚   â”œâ”€â”€ read_data_from_postgres.py
â”‚   â””â”€â”€ store_to_postgres.py
â”œâ”€â”€ stock_scraper/
â”‚   â”œâ”€â”€ spiders/                # Scrapy spiders for each feed/source
â”‚   â”œâ”€â”€ items.py
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ run_spiders.sh              # Entrypoint script to launch all scrapers
â”œâ”€â”€ .env                        # Environment variables (e.g. list of tickers)
â”œâ”€â”€ docker-compose.yml          # Defines multi-service stack (scraper + Postgres)
â”œâ”€â”€ requirements.txt            # Full environment dependencies (runtime)
â””â”€â”€ README.md                   # â† You are here!
```

## ğŸš€ Features

- **Multiple RSS sources**: Polls Top Stories, Realtime Headlines, Market Bulletins, and more.  
- **Extensible spiders**: Easily add new sources under `stock_scraper/spiders/`.  
- **Docker-Compose stack**: One-command bring-up for scraper + PostgreSQL.  
- **Automated storage**: Parsed items are inserted into PostgreSQL.  
- **Local JSON backup**: Scraped headlines also saved to `data/news.json`.

## ğŸ’» Prerequisites

- Docker & Docker Compose  
- Python 3.12+  
- A Python virtual environment (recommended)

## ğŸ”§ Getting Started

1. **Clone the repo**  
   ```bash
   git clone https://github.com/AlexBoyev/stock-news-scraper.git
   cd stock-news-scraper
   ```

2. **Set up a Python virtual environment**  
   ```bash
   python -m venv .venv
   # Activate it:
   source .venv/bin/activate    # Linux/macOS
   .\.venv\Scripts\activate  # Windows PowerShell
   ```

3. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**  
   - Copy `.env.example` (if provided) to `.env`  
   - Edit `.env` to define your list of tickers, database credentials, etc.

5. **Launch with Docker Compose**  
   ```bash
   docker-compose down -v
   docker-compose up -d --build
   docker-compose logs -f scraper
   ```

6. **Inspect Data**  
   - **PostgreSQL**:  
     ```bash
     docker exec -it <postgres_container> psql -U $POSTGRES_USER -d $POSTGRES_DB
     SELECT * FROM headlines LIMIT 10;
     ```  
   - **Local JSON**:  
     ```bash
     cat data/news.json
     ```

## ğŸ“ Adding a New Spider

1. Create a new Python module under `stock_scraper/spiders/`.  
2. Subclass `scrapy.Spider` and define your `start_urls`.  
3. Update `run_spiders.sh` to include your spider name.  
4. Rebuild or restart your Docker container.

## ğŸ¤ Contributing

- Fork the project  
- Create a feature branch  
- Submit a pull request with tests and documentation updates  

## ğŸ“„ License

MIT Â© Your Name
